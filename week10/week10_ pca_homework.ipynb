{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 - PCA and Dimension Reduction Homework\n",
    "Execute the below code and answer the following questions. __Do NOT commit the csv file!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def generate_data():\n",
    "    x, y = make_classification(n_samples=1500, \n",
    "                            n_features = 20,\n",
    "                            n_informative = 8,\n",
    "                            n_redundant = 5,\n",
    "                            n_repeated = 1, \n",
    "                            n_classes = 3,\n",
    "                            weights = (0.5, 0.25, 0.25),\n",
    "                            random_state = 120\n",
    "                            )\n",
    "    colNames = ['var'+str(x) for x in range(20)]\n",
    "    colNames.append('target')\n",
    "\n",
    "    df = pd.DataFrame(np.concatenate((x,y.reshape(-1,1)), axis=1), columns=colNames)\n",
    "    df.to_csv('pca-dataset.csv', index=False)\n",
    "    \n",
    "generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var0</th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "      <th>var4</th>\n",
       "      <th>var5</th>\n",
       "      <th>var6</th>\n",
       "      <th>var7</th>\n",
       "      <th>var8</th>\n",
       "      <th>var9</th>\n",
       "      <th>...</th>\n",
       "      <th>var11</th>\n",
       "      <th>var12</th>\n",
       "      <th>var13</th>\n",
       "      <th>var14</th>\n",
       "      <th>var15</th>\n",
       "      <th>var16</th>\n",
       "      <th>var17</th>\n",
       "      <th>var18</th>\n",
       "      <th>var19</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.882513</td>\n",
       "      <td>-3.272465</td>\n",
       "      <td>-2.520732</td>\n",
       "      <td>-1.987174</td>\n",
       "      <td>-2.073689</td>\n",
       "      <td>-3.272465</td>\n",
       "      <td>-1.237969</td>\n",
       "      <td>1.690547</td>\n",
       "      <td>-0.211314</td>\n",
       "      <td>-5.753190</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.574979</td>\n",
       "      <td>-1.916275</td>\n",
       "      <td>-5.994075</td>\n",
       "      <td>-3.349615</td>\n",
       "      <td>-0.846193</td>\n",
       "      <td>2.491347</td>\n",
       "      <td>1.360958</td>\n",
       "      <td>-2.892522</td>\n",
       "      <td>-1.377561</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.775242</td>\n",
       "      <td>-1.015994</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>0.057274</td>\n",
       "      <td>0.590205</td>\n",
       "      <td>-1.015994</td>\n",
       "      <td>1.350954</td>\n",
       "      <td>-1.493037</td>\n",
       "      <td>-0.862391</td>\n",
       "      <td>-1.986047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523760</td>\n",
       "      <td>0.399579</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.718606</td>\n",
       "      <td>-1.112030</td>\n",
       "      <td>0.083929</td>\n",
       "      <td>0.606544</td>\n",
       "      <td>-1.376793</td>\n",
       "      <td>1.302641</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.876376</td>\n",
       "      <td>0.220453</td>\n",
       "      <td>3.114224</td>\n",
       "      <td>-1.640025</td>\n",
       "      <td>1.180348</td>\n",
       "      <td>0.220453</td>\n",
       "      <td>0.465102</td>\n",
       "      <td>0.222511</td>\n",
       "      <td>0.880455</td>\n",
       "      <td>2.922315</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.370516</td>\n",
       "      <td>3.585262</td>\n",
       "      <td>-2.168162</td>\n",
       "      <td>2.693429</td>\n",
       "      <td>-0.966636</td>\n",
       "      <td>1.586302</td>\n",
       "      <td>-2.821546</td>\n",
       "      <td>0.482164</td>\n",
       "      <td>0.187404</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.550342</td>\n",
       "      <td>-1.968144</td>\n",
       "      <td>0.077681</td>\n",
       "      <td>-1.887719</td>\n",
       "      <td>1.864445</td>\n",
       "      <td>-1.968144</td>\n",
       "      <td>-0.527958</td>\n",
       "      <td>-0.201467</td>\n",
       "      <td>-0.532649</td>\n",
       "      <td>2.287445</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041341</td>\n",
       "      <td>2.383582</td>\n",
       "      <td>-0.417253</td>\n",
       "      <td>1.305379</td>\n",
       "      <td>-0.435123</td>\n",
       "      <td>-0.468557</td>\n",
       "      <td>0.923290</td>\n",
       "      <td>3.880050</td>\n",
       "      <td>2.676798</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.454974</td>\n",
       "      <td>1.293300</td>\n",
       "      <td>0.112201</td>\n",
       "      <td>-0.589989</td>\n",
       "      <td>-1.674321</td>\n",
       "      <td>1.293300</td>\n",
       "      <td>0.487302</td>\n",
       "      <td>1.776318</td>\n",
       "      <td>0.702520</td>\n",
       "      <td>-1.024127</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.452869</td>\n",
       "      <td>-0.667306</td>\n",
       "      <td>0.345364</td>\n",
       "      <td>-3.920591</td>\n",
       "      <td>-0.438296</td>\n",
       "      <td>-1.690141</td>\n",
       "      <td>0.176906</td>\n",
       "      <td>1.920142</td>\n",
       "      <td>1.474634</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       var0      var1      var2      var3      var4      var5      var6  \\\n",
       "0 -2.882513 -3.272465 -2.520732 -1.987174 -2.073689 -3.272465 -1.237969   \n",
       "1  0.775242 -1.015994  0.005137  0.057274  0.590205 -1.015994  1.350954   \n",
       "2 -0.876376  0.220453  3.114224 -1.640025  1.180348  0.220453  0.465102   \n",
       "3 -2.550342 -1.968144  0.077681 -1.887719  1.864445 -1.968144 -0.527958   \n",
       "4 -0.454974  1.293300  0.112201 -0.589989 -1.674321  1.293300  0.487302   \n",
       "\n",
       "       var7      var8      var9  ...     var11     var12     var13     var14  \\\n",
       "0  1.690547 -0.211314 -5.753190  ... -0.574979 -1.916275 -5.994075 -3.349615   \n",
       "1 -1.493037 -0.862391 -1.986047  ...  0.523760  0.399579  0.088600  0.718606   \n",
       "2  0.222511  0.880455  2.922315  ... -0.370516  3.585262 -2.168162  2.693429   \n",
       "3 -0.201467 -0.532649  2.287445  ... -0.041341  2.383582 -0.417253  1.305379   \n",
       "4  1.776318  0.702520 -1.024127  ... -0.452869 -0.667306  0.345364 -3.920591   \n",
       "\n",
       "      var15     var16     var17     var18     var19  target  \n",
       "0 -0.846193  2.491347  1.360958 -2.892522 -1.377561     0.0  \n",
       "1 -1.112030  0.083929  0.606544 -1.376793  1.302641     2.0  \n",
       "2 -0.966636  1.586302 -2.821546  0.482164  0.187404     0.0  \n",
       "3 -0.435123 -0.468557  0.923290  3.880050  2.676798     1.0  \n",
       "4 -0.438296 -1.690141  0.176906  1.920142  1.474634     0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('pca-dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 21 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   var0    1500 non-null   float64\n",
      " 1   var1    1500 non-null   float64\n",
      " 2   var2    1500 non-null   float64\n",
      " 3   var3    1500 non-null   float64\n",
      " 4   var4    1500 non-null   float64\n",
      " 5   var5    1500 non-null   float64\n",
      " 6   var6    1500 non-null   float64\n",
      " 7   var7    1500 non-null   float64\n",
      " 8   var8    1500 non-null   float64\n",
      " 9   var9    1500 non-null   float64\n",
      " 10  var10   1500 non-null   float64\n",
      " 11  var11   1500 non-null   float64\n",
      " 12  var12   1500 non-null   float64\n",
      " 13  var13   1500 non-null   float64\n",
      " 14  var14   1500 non-null   float64\n",
      " 15  var15   1500 non-null   float64\n",
      " 16  var16   1500 non-null   float64\n",
      " 17  var17   1500 non-null   float64\n",
      " 18  var18   1500 non-null   float64\n",
      " 19  var19   1500 non-null   float64\n",
      " 20  target  1500 non-null   float64\n",
      "dtypes: float64(21)\n",
      "memory usage: 246.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1,200\n",
      "Test samples: 300\n",
      "\n",
      "Features:\n",
      "var0\tvar1\tvar2\tvar3\tvar4\tvar5\tvar6\tvar7\tvar8\tvar9\tvar10\tvar11\tvar12\tvar13\tvar14\tvar15\tvar16\tvar17\tvar18\tvar19\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[[x for x in df.columns if x.startswith('var')]]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_training, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "print(f'Training samples: {X_train.shape[0]:,}')\n",
    "print(f'Test samples: {X_test.shape[0]:,}')\n",
    "\n",
    "print('\\nFeatures:')\n",
    "print(*X_train, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "- `var1 - var19`: a feature for the data.  \n",
    "- `target`: variable we wish to be able to predict, which is 1 of 3 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "- Use principle components analysis to determine the number of components to reduce the data to by evaluating the explained variance ratio (use `X_train`).  \n",
    "- Remember to scale the data first.  \n",
    "- What number of components would you recommend based on your analysis?  \n",
    "- Explain your results using markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# insert code here\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "pca = PCA().fit(X_train_scaled)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Based on the analysis of the explained variance ratio, I would recommend reducing the data to 10 components. At this point, approximately 95% of the variance in the data is explained, and there is a significant drop in the amount of variance explained by each additional component beyond 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Insert comments>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "- Evaluate the target variable in the `df` object.  \n",
    "- Which metric would you use in evaluating a predictive model. Explain your choice in the markdown cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    744\n",
       "2.0    380\n",
       "1.0    376\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert code here\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1500.000000\n",
       "mean        0.757333\n",
       "std         0.831208\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         1.000000\n",
       "75%         2.000000\n",
       "max         2.000000\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It's difficult to recommend a single metric without knowing the specific problem and goals. However, if I had to choose one metric, I would choose the F1-score. The F1-score balances precision and recall and provides a single score that summarizes the performance of the model. It is useful when there is an imbalance between the classes, and it is a good metric to use when both false positives and false negatives are important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "- Without using PCA, create a logistic regression model using practices discussed in class.  \n",
    "- Which model would you choose? Explain your results in the markdown cells.    \n",
    "- What is the accuracy, precision, and recall for the test data?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=123)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert code here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "lr = LogisticRegression(random_state=123)\n",
    "lr.fit(X_train, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7033333333333334\n",
      "Precision: 0.7040254342431762\n",
      "Recall: 0.7033333333333334\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The choice of the best model depends on the context and the specific needs of the problem. In our case, we can compare the logistic regression model's performance to a baseline model, such as a random classifier, to determine if it is better than random. We can also compare the performance of the logistic regression model with other models, such as decision trees, random forests, or support vector machines, to see if it is the best model for the specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the logistic regression model without using PCA is 70.3% and the precision is 70.4% also the recall is 70.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "- Use PCA within a pipeline to create a logistic regression model using best practices from class.  \n",
    "- Which model performs the best on the training data? Explain your results in markdown cells.  \n",
    "- What is the accuracy, precision, and recall for the test data?\n",
    "- Does this perform better than the original logistic regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('pca', PCA(n_components=10)), \n",
    "    ('logreg', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=10)),\n",
       "                ('logreg', LogisticRegression())])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "Precision: 0.6872885414018138\n",
      "Recall: 0.69\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From the above scores, we can observe that the model without PCA performs slightly better than the model with PCA in terms of accuracy and precision. PCA is a dimensionality reduction technique that reduces the number of features in the dataset. Although it can help to simplify the dataset and reduce computation time, it can also lead to some loss of information. In some cases, this loss of information can impact the performance of the model. In this scenario, since the difference in performance between the two models is not significant, we can conclude that the use of PCA does not have a significant impact on the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Without using PCA, create a decision tree model using best practices discussed in class.  \n",
    "- Which model performs the best on the training data? Explain your results in the markdown cells.  \n",
    "- What is the accuracy, precision, and recall for the test data?  \n",
    "- Does this perform better than either of the logistic regression models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, random_state=123)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert code here\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=123)\n",
    "dt.fit(X_train, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6866666666666666\n",
      "Precision: 0.6877344414943697\n",
      "Recall: 0.6866666666666666\n"
     ]
    }
   ],
   "source": [
    "y_pred = dt.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As per the above results of the two models that were evaluated, Logistic regression model performed better than decision tree model in the training data as the accuracy, precision and recall is higher to the Logistic regression model without using PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "- Repeat `Question 5` but use PCA.  \n",
    "- Does this perform better than the original Decision Tree or the logistic regression models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, random_state=123)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.fit(X_train_pca, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the testing data using the same PCA transformation\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5966666666666667\n",
      "Precision: 0.5850750304215485\n",
      "Recall: 0.5966666666666667\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model on the transformed testing data\n",
    "y_pred = dt.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Based on the performance metrics of the decision tree model using PCA on the dataset provided in the previous questions, it has a slightly lower accuracy, precision, and recall than the original decision tree model and also the original logistic regression model. So, as I mentioned above the original machine learning models are performing better than using PCA with models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
